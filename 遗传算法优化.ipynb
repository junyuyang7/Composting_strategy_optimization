{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找最优模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 模型的决定系数的文件\n",
    "file_paths = [\n",
    "    r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_TN loss  (%).json\",\n",
    "    r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_CH4-C loss (%).json\",\n",
    "    r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_CO2-C loss  (%).json\",\n",
    "    r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_N2O-N loss  (%).json\",\n",
    "    r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_TC loss  (%).json\",\n",
    "    r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_NH3-N loss  (%).json\",\n",
    "]\n",
    "\n",
    "# 模型名称列表\n",
    "model_names = [\n",
    "    'ctb_model',\n",
    "    'gp_model',\n",
    "    'lgb_model',\n",
    "    'lr_model',\n",
    "    'mlp_model',\n",
    "    'rf_model',\n",
    "    'ri_model',\n",
    "    'svr_model',\n",
    "    'xgb_model'\n",
    "]\n",
    "\n",
    "# 模型缩写映射\n",
    "model_mapping = {\n",
    "    \"RandomForest(k)\": \"rf_model\",\n",
    "    \"XGBoost(k)\": \"xgb_model\",\n",
    "    \"Lightgbm(k)\": \"lgb_model\",\n",
    "    \"CatRegression(k)\": \"ctb_model\",\n",
    "    \"RidgeRegression(k)\": \"ri_model\",\n",
    "    \"LinearRegression(k)\": \"lr_model\",\n",
    "    \"MLP(k)\": \"mlp_model\",\n",
    "    \"SVR(k)\": \"svr_model\",\n",
    "    \"GaussR(k)\": \"gp_model\"  # 假设 GaussR 对应 gp_model\n",
    "}\n",
    "\n",
    "\n",
    "# 损失类型对应的目录路径\n",
    "model_paths = {\n",
    "    'result_r2_TN loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_TN loss  (%)',\n",
    "    'result_r2_CH4-C loss (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_CH4-C loss (%)',\n",
    "    'result_r2_CO2-C loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_CO2-C loss  (%)',\n",
    "    'result_r2_N2O-N loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_N2O-N loss  (%)',\n",
    "    'result_r2_TC loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_NH3-N loss  (%)',\n",
    "    'result_r2_NH3-N loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_TC loss  (%)'\n",
    "}\n",
    "\n",
    "# CSV 文件路径\n",
    "csv_files = [\n",
    "    \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_CH4-C loss (%).csv\",\n",
    "    \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_CO2-C loss  (%).csv\",\n",
    "    \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_N2O-N loss  (%).csv\",\n",
    "    \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_NH3-N loss  (%).csv\",\n",
    "    \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_TC loss  (%).csv\",\n",
    "    \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_TN loss  (%).csv\",\n",
    "]\n",
    "\n",
    "non_nums  = ['material_0', 'material_1', 'Excipients', 'Additive Species']\n",
    "labels = [ 'TN loss  (%)','CH4-C loss (%)',  'CO2-C loss  (%)', \"N2O-N loss  (%)\", 'NH3-N loss  (%)', 'TC loss  (%)', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个文件的最优模型: {'result_r2_TN loss  (%).json': 'Lightgbm(k)', 'result_r2_CH4-C loss (%).json': 'SVR(k)', 'result_r2_CO2-C loss  (%).json': 'CatRegression(k)', 'result_r2_N2O-N loss  (%).json': 'Lightgbm(k)', 'result_r2_TC loss  (%).json': 'Lightgbm(k)', 'result_r2_NH3-N loss  (%).json': 'Lightgbm(k)'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "# 忽略所有警告\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# file_paths = [\n",
    "#     r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_TN loss  (%).json\",\n",
    "#     r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_CH4-C loss (%).json\",\n",
    "#     r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_CO2-C loss  (%).json\",\n",
    "#     r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_N2O-N loss  (%).json\",\n",
    "#     r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_TC loss  (%).json\",\n",
    "#     r\"output\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\result_r2_NH3-N loss  (%).json\",\n",
    "# ]\n",
    "\n",
    "optimal_models = {}  # 创建一个空字典来存储每个文件的最优模型\n",
    "\n",
    "# 遍历每个文件\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        # min_value = float('inf')\n",
    "        min_value = -float('inf')\n",
    "        min_key = None\n",
    "        # 查找最大值的键\n",
    "        for key, value in data.items():\n",
    "            if value > min_value:\n",
    "                min_key = key\n",
    "                min_value = value\n",
    "        # 更新字典，文件名作为键，最优模型名作为值\n",
    "        optimal_models[os.path.basename(file_path)] = min_key\n",
    "\n",
    "# 打印每个文件的最优模型\n",
    "print(\"每个文件的最优模型:\", optimal_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载相应的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建模型路径字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型缩写映射\n",
    "# model_mapping = {\n",
    "#     \"RandomForest(k)\": \"rf_model\",\n",
    "#     \"XGBoost(k)\": \"xgb_model\",\n",
    "#     \"Lightgbm(k)\": \"lgb_model\",\n",
    "#     \"CatRegression(k)\": \"ctb_model\",\n",
    "#     \"RidgeRegression(k)\": \"ri_model\",\n",
    "#     \"LinearRegression(k)\": \"lr_model\",\n",
    "#     \"MLP(k)\": \"mlp_model\",\n",
    "#     \"SVR(k)\": \"svr_model\",\n",
    "#     \"GaussR(k)\": \"gp_model\"  # 假设 GaussR 对应 gp_model\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型文件路径列表： ['output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_TN loss  (%)\\\\lgb_model.pkl', 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_CH4-C loss (%)\\\\svr_model.pkl', 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_CO2-C loss  (%)\\\\ctb_model.pkl', 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_N2O-N loss  (%)\\\\lgb_model.pkl', 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_NH3-N loss  (%)\\\\lgb_model.pkl', 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_TC loss  (%)\\\\lgb_model.pkl']\n"
     ]
    }
   ],
   "source": [
    "# # 损失类型对应的目录路径\n",
    "# model_paths = {\n",
    "#     'result_r2_TN loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_TN loss  (%)',\n",
    "#     'result_r2_CH4-C loss (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_CH4-C loss (%)',\n",
    "#     'result_r2_CO2-C loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_CO2-C loss  (%)',\n",
    "#     'result_r2_N2O-N loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_N2O-N loss  (%)',\n",
    "#     'result_r2_TC loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_NH3-N loss  (%)',\n",
    "#     'result_r2_NH3-N loss  (%).json': 'output\\\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\\\model_TC loss  (%)'\n",
    "# }\n",
    "\n",
    "# 存储模型文件路径的列表\n",
    "model_files = []\n",
    "\n",
    "# 遍历optimal_models字典\n",
    "for key, value in optimal_models.items():\n",
    "    # 获取模型名字\n",
    "    model_name = model_mapping.get(value)\n",
    "    model_path_dir = model_paths.get(key)\n",
    "    # 如果模型名存在\n",
    "    if model_name:\n",
    "        # 拼接模型文件路径\n",
    "        model_file_path = os.path.join(model_path_dir, model_name + \".pkl\",)\n",
    "        \n",
    "        # 检查文件是否存在\n",
    "        if os.path.exists(model_file_path):\n",
    "            model_files.append(model_file_path)\n",
    "        else:\n",
    "            print(f\"找不到模型文件：{model_file_path}\")\n",
    "    else:\n",
    "        print(f\"找不到与模型键 {key} 相关的模型映射\")\n",
    "\n",
    "# 打印模型文件路径列表\n",
    "print(\"模型文件路径列表：\", model_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_TN loss  (%)_lgb_model\n",
      "model_CH4-C loss (%)_svr_model\n",
      "model_CO2-C loss  (%)_ctb_model\n",
      "model_N2O-N loss  (%)_lgb_model\n",
      "model_NH3-N loss  (%)_lgb_model\n",
      "model_TC loss  (%)_lgb_model\n",
      "{'model_TN loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_CH4-C loss (%)_svr_model': SVR(), 'model_CO2-C loss  (%)_ctb_model': <catboost.core.CatBoostRegressor object at 0x0000020D351C8850>, 'model_N2O-N loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_NH3-N loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_TC loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1)}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "loaded_models = {}  # 用于存储加载的模型\n",
    "\n",
    "# 遍历每个模型文件\n",
    "for model_file in model_files:\n",
    "    # 获取模型名称\n",
    "    model_name = os.path.basename(os.path.dirname(model_file)) + \"_\" + os.path.splitext(os.path.basename(model_file))[0]\n",
    "    print(model_name)\n",
    "    # 加载模型\n",
    "    with open(model_file, 'rb') as file:\n",
    "        loaded_model = joblib.load(file)\n",
    "    \n",
    "    # 将加载的模型存储在字典中，以模型名称作为键\n",
    "    loaded_models[model_name] = loaded_model\n",
    "print(loaded_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取每个模型的输入和输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型名称: data_for_CH4-C loss (%)\n",
      "输入特征: ['material_0', 'initial TN(%)', 'initial TC(%)', 'initial moisture content(%)', 'initial CN(%)', 'initial pH', 'material_1', 'Excipients_1', 'Additive Species']\n",
      "输出特征: CH4-C loss (%)\n",
      "模型名称: data_for_CO2-C loss  (%)\n",
      "输入特征: ['material_0', 'initial TN(%)', 'initial TC(%)', 'initial moisture content(%)', 'initial CN(%)', 'initial pH', 'material_1', 'Excipients_1', 'Additive Species']\n",
      "输出特征: CO2-C loss  (%)\n",
      "模型名称: data_for_N2O-N loss  (%)\n",
      "输入特征: ['material_0', 'initial TN(%)', 'initial TC(%)', 'initial moisture content(%)', 'initial CN(%)', 'initial pH', 'material_1', 'Excipients_1', 'Additive Species']\n",
      "输出特征: N2O-N loss  (%)\n",
      "模型名称: data_for_NH3-N loss  (%)\n",
      "输入特征: ['material_0', 'initial TN(%)', 'initial TC(%)', 'initial moisture content(%)', 'initial CN(%)', 'initial pH', 'material_1', 'Excipients_1', 'Additive Species']\n",
      "输出特征: NH3-N loss  (%)\n",
      "模型名称: data_for_TC loss  (%)\n",
      "输入特征: ['material_0', 'initial TN(%)', 'initial TC(%)', 'initial moisture content(%)', 'initial CN(%)', 'initial pH', 'material_1', 'Excipients_1', 'Additive Species']\n",
      "输出特征: TC loss  (%)\n",
      "模型名称: data_for_TN loss  (%)\n",
      "输入特征: ['material_0', 'initial TN(%)', 'initial TC(%)', 'initial moisture content(%)', 'initial CN(%)', 'initial pH', 'material_1', 'Excipients_1', 'Additive Species']\n",
      "输出特征: TN loss  (%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# # CSV 文件路径\n",
    "# csv_files = [\n",
    "#     \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_CH4-C loss (%).csv\",\n",
    "#     \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_CO2-C loss  (%).csv\",\n",
    "#     \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_N2O-N loss  (%).csv\",\n",
    "#     \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_NH3-N loss  (%).csv\",\n",
    "#     \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_TC loss  (%).csv\",\n",
    "#     \"data\\TN_NH3_N2O_TC loss_CH4-C loss_CO2-C loss\\data_for_TN loss  (%).csv\",\n",
    "# ]\n",
    "\n",
    "# 用于存储输入特征和输出特征的列表\n",
    "input_features_list = []\n",
    "output_feature_list = []\n",
    "model_names = []\n",
    "\n",
    "# 遍历每个 CSV 文件\n",
    "for csv_file in csv_files:\n",
    "    # 从文件路径中提取模型名称\n",
    "    model_name = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    model_names.append(model_name)\n",
    "    \n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # 获取输入特征（除了最后一列）和输出特征（最后一列）的列名，并添加到列表中\n",
    "    input_features = list(df.columns[:-1])\n",
    "    output_feature = df.columns[-1]\n",
    "    input_features_list.append(input_features)\n",
    "    output_feature_list.append(output_feature)\n",
    "\n",
    "# 打印模型名称、输入特征和输出特征的列表\n",
    "for model_name, input_features, output_feature in zip(model_names, input_features_list, output_feature_list):\n",
    "    print(\"模型名称:\", model_name)\n",
    "    print(\"输入特征:\", input_features)\n",
    "    print(\"输出特征:\", output_feature)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找输入输出的取值范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "material_0: Minimum=0, Maximum=6\n",
      "initial TN(%): Minimum=-1.0, Maximum=11.58\n",
      "initial TC(%): Minimum=-1.0, Maximum=197.0\n",
      "initial moisture content(%): Minimum=-1.0, Maximum=89.8\n",
      "initial CN(%): Minimum=-1.0, Maximum=55.98\n",
      "initial pH: Minimum=-1.0, Maximum=10.7\n",
      "material_1: Minimum=0, Maximum=11\n",
      "Excipients_1: Minimum=0, Maximum=58\n",
      "Additive Species: Minimum=0, Maximum=4\n",
      "CH4-C loss (%): Minimum=0.0, Maximum=35.87\n",
      "CO2-C loss  (%): Minimum=-0.8, Maximum=84.0\n",
      "N2O-N loss  (%): Minimum=-0.5, Maximum=19.0\n",
      "NH3-N loss  (%): Minimum=0.02, Maximum=160.6\n",
      "TC loss  (%): Minimum=5.1, Maximum=92.58\n",
      "TN loss  (%): Minimum=-1.0, Maximum=85.54\n",
      "{'material_0': {'Minimum': 0, 'Maximum': 6}, 'initial TN(%)': {'Minimum': -1.0, 'Maximum': 11.58}, 'initial TC(%)': {'Minimum': -1.0, 'Maximum': 197.0}, 'initial moisture content(%)': {'Minimum': -1.0, 'Maximum': 89.8}, 'initial CN(%)': {'Minimum': -1.0, 'Maximum': 55.98}, 'initial pH': {'Minimum': -1.0, 'Maximum': 10.7}, 'material_1': {'Minimum': 0, 'Maximum': 11}, 'Excipients_1': {'Minimum': 0, 'Maximum': 58}, 'Additive Species': {'Minimum': 0, 'Maximum': 4}, 'CH4-C loss (%)': {'Minimum': 0.0, 'Maximum': 35.87}, 'CO2-C loss  (%)': {'Minimum': -0.8, 'Maximum': 84.0}, 'N2O-N loss  (%)': {'Minimum': -0.5, 'Maximum': 19.0}, 'NH3-N loss  (%)': {'Minimum': 0.02, 'Maximum': 160.6}, 'TC loss  (%)': {'Minimum': 5.1, 'Maximum': 92.58}, 'TN loss  (%)': {'Minimum': -1.0, 'Maximum': 85.54}}\n"
     ]
    }
   ],
   "source": [
    "# 存储所有输入特征和标签的最小值和最大值的字典\n",
    "min_max_values = {}\n",
    "\n",
    "# 遍历每个 CSV 文件\n",
    "for csv_file in csv_files:\n",
    "    # 读取 CSV 文件\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # 获取输入特征的列名（去除最后一列）\n",
    "    input_features = list(df.columns[:-1])\n",
    "    # 获取标签的列名\n",
    "    label_column = df.columns[-1]\n",
    "    \n",
    "    # 遍历每个输入特征\n",
    "    for feature in input_features:\n",
    "        # 如果特征不存在，创建新的特征项\n",
    "        if feature not in min_max_values:\n",
    "            min_max_values[feature] = {'Minimum': None, 'Maximum': None}\n",
    "        # 计算输入特征的最小值和最大值，并更新字典中的值\n",
    "        if min_max_values[feature]['Minimum'] is None:\n",
    "            min_max_values[feature]['Minimum'] = round(df[feature].min(), 2)\n",
    "        else:\n",
    "            min_max_values[feature]['Minimum'] = min(round(min_max_values[feature]['Minimum'], 2), round(df[feature].min(), 2))\n",
    "        if min_max_values[feature]['Maximum'] is None:\n",
    "            min_max_values[feature]['Maximum'] = round(df[feature].max(), 2)\n",
    "        else:\n",
    "            min_max_values[feature]['Maximum'] = max(round(min_max_values[feature]['Maximum'], 2), round(df[feature].max(), 2))\n",
    "    \n",
    "    # 如果标签不存在，创建新的标签项\n",
    "    if label_column not in min_max_values:\n",
    "        min_max_values[label_column] = {'Minimum': None, 'Maximum': None}\n",
    "    # 计算标签的最小值和最大值，并更新字典中的值\n",
    "    if min_max_values[label_column]['Minimum'] is None:\n",
    "        min_max_values[label_column]['Minimum'] = round(df[label_column].min(), 2)\n",
    "    else:\n",
    "        min_max_values[label_column]['Minimum'] = min(round(min_max_values[label_column]['Minimum'], 2), round(df[label_column].min(), 2))\n",
    "    if min_max_values[label_column]['Maximum'] is None:\n",
    "        min_max_values[label_column]['Maximum'] = round(df[label_column].max(), 2)\n",
    "    else:\n",
    "        min_max_values[label_column]['Maximum'] = max(round(min_max_values[label_column]['Maximum'], 2), round(df[label_column].max(), 2))\n",
    "\n",
    "# 打印每个输入特征和标签的最小值和最大值\n",
    "for category, stats in min_max_values.items():\n",
    "    print(f\"{category}: Minimum={stats['Minimum']}, Maximum={stats['Maximum']}\")\n",
    "\n",
    "print(min_max_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗传算法优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_N2O-N loss (%)_ctb_model优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CH4-C loss (%)\n",
      "SVR()\n",
      "{'model_TN loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_CH4-C loss (%)_svr_model': SVR(), 'model_CO2-C loss  (%)_ctb_model': <catboost.core.CatBoostRegressor object at 0x0000020D353A1950>, 'model_N2O-N loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_NH3-N loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_TC loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1)}\n",
      "Input Ranges:\n",
      "{'material_0': {'Minimum': 0, 'Maximum': 6}, 'initial TN(%)': {'Minimum': -1.0, 'Maximum': 11.58}, 'initial TC(%)': {'Minimum': -1.0, 'Maximum': 197.0}, 'initial moisture content(%)': {'Minimum': -1.0, 'Maximum': 89.8}, 'initial CN(%)': {'Minimum': -1.0, 'Maximum': 55.98}, 'initial pH': {'Minimum': -1.0, 'Maximum': 10.7}, 'material_1': {'Minimum': 0, 'Maximum': 11}, 'Excipients_1': {'Minimum': 0, 'Maximum': 58}, 'Additive Species': {'Minimum': 0, 'Maximum': 4}}\n",
      "\n",
      "Output Ranges:\n",
      "{'CH4-C loss (%)': {'Minimum': 0.0, 'Maximum': 35.87}, 'CO2-C loss  (%)': {'Minimum': -0.8, 'Maximum': 84.0}, 'N2O-N loss  (%)': {'Minimum': -0.5, 'Maximum': 19.0}, 'NH3-N loss  (%)': {'Minimum': 0.02, 'Maximum': 160.6}, 'TC loss  (%)': {'Minimum': 5.1, 'Maximum': 92.58}, 'TN loss  (%)': {'Minimum': -1.0, 'Maximum': 85.54}}\n",
      "Run 1: Best individual with fitness [-1.37829855]\n",
      "Run 2: Best individual with fitness [-1.4337747]\n",
      "Run 3: Best individual with fitness [-1.24088205]\n",
      "Run 4: Best individual with fitness [-1.57597103]\n",
      "Run 5: Best individual with fitness [-1.42442736]\n",
      "Run 6: Best individual with fitness [-1.31102456]\n",
      "Run 7: Best individual with fitness [-1.21869752]\n",
      "Run 8: Best individual with fitness [-1.34128626]\n",
      "Run 9: Best individual with fitness [-1.25003278]\n",
      "Run 10: Best individual with fitness [-1.31090565]\n",
      "Run 11: Best individual with fitness [-1.45042406]\n",
      "Run 12: Best individual with fitness [-1.26277759]\n",
      "Run 13: Best individual with fitness [-1.27177994]\n",
      "Run 14: Best individual with fitness [-1.64005165]\n",
      "Run 15: Best individual with fitness [-1.19502331]\n",
      "Run 16: Best individual with fitness [-1.34407195]\n",
      "Run 17: Best individual with fitness [-1.41025828]\n",
      "Run 18: Best individual with fitness [-1.31521349]\n",
      "Run 19: Best individual with fitness [-1.53008424]\n",
      "Run 20: Best individual with fitness [-1.35271243]\n",
      "Run 21: Best individual with fitness [-1.13234477]\n",
      "Run 22: Best individual with fitness [-1.49900931]\n",
      "Run 23: Best individual with fitness [-1.21796]\n",
      "Run 24: Best individual with fitness [-1.18901393]\n",
      "Run 25: Best individual with fitness [-1.35576194]\n",
      "Run 26: Best individual with fitness [-1.62586205]\n",
      "Run 27: Best individual with fitness [-1.28344252]\n",
      "Run 28: Best individual with fitness [-1.60894714]\n",
      "Run 29: Best individual with fitness [-1.34081829]\n",
      "Run 30: Best individual with fitness [-1.37361977]\n",
      "Run 31: Best individual with fitness [-1.06113841]\n",
      "Run 32: Best individual with fitness [-1.21886986]\n",
      "Run 33: Best individual with fitness [-1.42348421]\n",
      "Run 34: Best individual with fitness [-1.25617062]\n",
      "Run 35: Best individual with fitness [-1.39661044]\n",
      "Run 36: Best individual with fitness [-1.31867345]\n",
      "Run 37: Best individual with fitness [-1.21998151]\n",
      "Run 38: Best individual with fitness [-1.34628474]\n",
      "Run 39: Best individual with fitness [-0.98197439]\n",
      "Run 40: Best individual with fitness [-1.18579616]\n",
      "Run 41: Best individual with fitness [-1.08165105]\n",
      "Run 42: Best individual with fitness [-1.53676506]\n",
      "Run 43: Best individual with fitness [-1.49145583]\n",
      "Run 44: Best individual with fitness [-1.13140194]\n",
      "Run 45: Best individual with fitness [-1.10591086]\n",
      "Run 46: Best individual with fitness [-1.24970304]\n",
      "Run 47: Best individual with fitness [-1.49331749]\n",
      "Run 48: Best individual with fitness [-1.48095219]\n",
      "Run 49: Best individual with fitness [-1.21507504]\n",
      "Run 50: Best individual with fitness [-1.31996933]\n",
      "Run 51: Best individual with fitness [-1.14774742]\n",
      "Run 52: Best individual with fitness [-1.49224258]\n",
      "Run 53: Best individual with fitness [-1.36963171]\n",
      "Run 54: Best individual with fitness [-1.06210985]\n",
      "Run 55: Best individual with fitness [-1.2954577]\n",
      "Run 56: Best individual with fitness [-0.95292203]\n",
      "Run 57: Best individual with fitness [-1.4480462]\n",
      "Run 58: Best individual with fitness [-1.32142861]\n",
      "Run 59: Best individual with fitness [-1.24007519]\n",
      "Run 60: Best individual with fitness [-1.15736841]\n",
      "Run 61: Best individual with fitness [-1.32982334]\n",
      "Run 62: Best individual with fitness [-1.40064247]\n",
      "Run 63: Best individual with fitness [-1.29989027]\n",
      "Run 64: Best individual with fitness [-1.69277977]\n",
      "Run 65: Best individual with fitness [-1.49872524]\n",
      "Run 66: Best individual with fitness [-1.37143245]\n",
      "Run 67: Best individual with fitness [-1.52152265]\n",
      "Run 68: Best individual with fitness [-1.19887443]\n",
      "Run 69: Best individual with fitness [-1.46576912]\n",
      "Run 70: Best individual with fitness [-1.33629824]\n",
      "Run 71: Best individual with fitness [-1.47962332]\n",
      "Run 72: Best individual with fitness [-1.2848607]\n",
      "Run 73: Best individual with fitness [-1.21396895]\n",
      "Run 74: Best individual with fitness [-1.48601269]\n",
      "Run 75: Best individual with fitness [-1.12045003]\n",
      "Run 76: Best individual with fitness [-1.4821182]\n",
      "Run 77: Best individual with fitness [-1.29745262]\n",
      "Run 78: Best individual with fitness [-1.44038664]\n",
      "Run 79: Best individual with fitness [-1.60844774]\n",
      "Run 80: Best individual with fitness [-1.5134565]\n",
      "Run 81: Best individual with fitness [-1.15278194]\n",
      "Run 82: Best individual with fitness [-1.15956684]\n",
      "Run 83: Best individual with fitness [-1.11370024]\n",
      "Run 84: Best individual with fitness [-1.30707517]\n",
      "Run 85: Best individual with fitness [-1.55682101]\n",
      "Run 86: Best individual with fitness [-1.20476223]\n",
      "Run 87: Best individual with fitness [-1.16645673]\n",
      "Run 88: Best individual with fitness [-1.18649156]\n",
      "Run 89: Best individual with fitness [-1.17936688]\n",
      "Run 90: Best individual with fitness [-1.36179013]\n",
      "Run 91: Best individual with fitness [-1.28575265]\n",
      "Run 92: Best individual with fitness [-1.46595842]\n",
      "Run 93: Best individual with fitness [-1.47013981]\n",
      "Run 94: Best individual with fitness [-1.22982766]\n",
      "Run 95: Best individual with fitness [-1.3389383]\n",
      "Run 96: Best individual with fitness [-1.46450832]\n",
      "Run 97: Best individual with fitness [-1.1456932]\n",
      "Run 98: Best individual with fitness [-1.48420618]\n",
      "Run 99: Best individual with fitness [-1.31242688]\n",
      "Run 100: Best individual with fitness [-1.38926058]\n",
      "Best individuals saved to_CH4-C loss (%): best_individuals_CH4-C loss (%).csv\n",
      "\n",
      "Combinations with minimum CH4-C loss (%) ([0.95292203]%):\n",
      "material_0                                        5\n",
      "initial TN(%)                              2.276926\n",
      "initial TC(%)                            102.222113\n",
      "initial moisture content(%)                75.26347\n",
      "initial CN(%)                             44.062667\n",
      "initial pH                                 0.239316\n",
      "material_1                                       11\n",
      "Excipients_1                               8.973939\n",
      "Additive Species                                  0\n",
      "CH4-C loss (%)                 [0.9529220253331191]\n",
      "Name: 55, dtype: object\n",
      "CO2-C loss  (%)\n",
      "<catboost.core.CatBoostRegressor object at 0x0000020D353A1950>\n",
      "{'model_TN loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_CH4-C loss (%)_svr_model': SVR(), 'model_CO2-C loss  (%)_ctb_model': <catboost.core.CatBoostRegressor object at 0x0000020D353A1950>, 'model_N2O-N loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_NH3-N loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1), 'model_TC loss  (%)_lgb_model': LGBMRegressor(metric='root_mean_squared_error', min_child_samples=10,\n",
      "              n_estimators=300, objective='regression', seed=2023, verbose=-1)}\n",
      "Input Ranges:\n",
      "{'material_0': {'Minimum': 0, 'Maximum': 6}, 'initial TN(%)': {'Minimum': -1.0, 'Maximum': 11.58}, 'initial TC(%)': {'Minimum': -1.0, 'Maximum': 197.0}, 'initial moisture content(%)': {'Minimum': -1.0, 'Maximum': 89.8}, 'initial CN(%)': {'Minimum': -1.0, 'Maximum': 55.98}, 'initial pH': {'Minimum': -1.0, 'Maximum': 10.7}, 'material_1': {'Minimum': 0, 'Maximum': 11}, 'Excipients_1': {'Minimum': 0, 'Maximum': 58}, 'Additive Species': {'Minimum': 0, 'Maximum': 4}}\n",
      "\n",
      "Output Ranges:\n",
      "{'CH4-C loss (%)': {'Minimum': 0.0, 'Maximum': 35.87}, 'CO2-C loss  (%)': {'Minimum': -0.8, 'Maximum': 84.0}, 'N2O-N loss  (%)': {'Minimum': -0.5, 'Maximum': 19.0}, 'NH3-N loss  (%)': {'Minimum': 0.02, 'Maximum': 160.6}, 'TC loss  (%)': {'Minimum': 5.1, 'Maximum': 92.58}, 'TN loss  (%)': {'Minimum': -1.0, 'Maximum': 85.54}}\n",
      "Run 1: Best individual with fitness [-28.44238396]\n",
      "Run 2: Best individual with fitness [-26.3083089]\n",
      "Run 3: Best individual with fitness [-27.58572945]\n",
      "Run 4: Best individual with fitness [-31.67879595]\n",
      "Run 5: Best individual with fitness [-27.20570006]\n",
      "Run 6: Best individual with fitness [-54.52463919]\n",
      "Run 7: Best individual with fitness [-38.08768291]\n",
      "Run 8: Best individual with fitness [-35.94361784]\n",
      "Run 9: Best individual with fitness [-32.57606022]\n",
      "Run 10: Best individual with fitness [-35.07092768]\n",
      "Run 11: Best individual with fitness [-27.44971659]\n",
      "Run 12: Best individual with fitness [-29.46046826]\n",
      "Run 13: Best individual with fitness [-40.05910694]\n",
      "Run 14: Best individual with fitness [-28.06002965]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m best_individuals \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[1;32m--> 113\u001b[0m     pop, stats, hof \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpopulation_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_generations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     best_individual \u001b[38;5;241m=\u001b[39m tools\u001b[38;5;241m.\u001b[39mselBest(pop, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Best individual with fitness \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_individual\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[128], line 99\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(population_size, n_generations, cxpb, mutpb)\u001b[0m\n\u001b[0;32m     96\u001b[0m stats \u001b[38;5;241m=\u001b[39m tools\u001b[38;5;241m.\u001b[39mStatistics(\u001b[38;5;28;01mlambda\u001b[39;00m ind: ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     97\u001b[0m stats\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mmin\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meaSimple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoolbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcxpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcxpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmutpb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmutpb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_generations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhalloffame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhof\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pop, stats, hof\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\SOCYIED\\Lib\\site-packages\\deap\\algorithms.py:173\u001b[0m, in \u001b[0;36meaSimple\u001b[1;34m(population, toolbox, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m invalid_ind \u001b[38;5;241m=\u001b[39m [ind \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m offspring \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ind\u001b[38;5;241m.\u001b[39mfitness\u001b[38;5;241m.\u001b[39mvalid]\n\u001b[0;32m    172\u001b[0m fitnesses \u001b[38;5;241m=\u001b[39m toolbox\u001b[38;5;241m.\u001b[39mmap(toolbox\u001b[38;5;241m.\u001b[39mevaluate, invalid_ind)\n\u001b[1;32m--> 173\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minvalid_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitnesses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitness\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# Update the hall of fame with the generated individuals\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[128], line 79\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(individual)\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m1e6\u001b[39m,)  \u001b[38;5;66;03m# 返回一个非常大的适应度值，表示不合法的个体\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# 使用模型进行预测\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindividual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# 计算模型输出并返回其负值作为适应度（因为我们是最小化问题）\u001b[39;00m\n\u001b[0;32m     82\u001b[0m fitness \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mprediction\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\SOCYIED\\Lib\\site-packages\\catboost\\core.py:5852\u001b[0m, in \u001b[0;36mCatBoostRegressor.predict\u001b[1;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[0;32m   5850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prediction_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5851\u001b[0m     prediction_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_default_prediction_type()\n\u001b[1;32m-> 5852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\SOCYIED\\Lib\\site-packages\\catboost\\core.py:2601\u001b[0m, in \u001b[0;36mCatBoost._predict\u001b[1;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[0;32m   2598\u001b[0m data, data_is_single_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_predict_input_data(data, parent_method_name, thread_count)\n\u001b[0;32m   2599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_prediction_type(prediction_type)\n\u001b[1;32m-> 2601\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predictions[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m data_is_single_object \u001b[38;5;28;01melse\u001b[39;00m predictions\n",
      "File \u001b[1;32mf:\\Anaconda\\envs\\SOCYIED\\Lib\\site-packages\\catboost\\core.py:1828\u001b[0m, in \u001b[0;36m_CatBoostBase._base_predict\u001b[1;34m(self, pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_base_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, pool, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type):\n\u001b[1;32m-> 1828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mntree_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# non_nums  = ['material_0', 'material_1', 'Excipients', 'Additive Species']\n",
    "# labels = [ 'TN loss  (%)','CH4-C loss (%)',  'CO2-C loss  (%)', \"N2O-N loss  (%)\", 'NH3-N loss  (%)', 'TC loss  (%)', ]\n",
    "\n",
    "#  SVR = array(['material_0', 'initial TN(%)', 'initial TC(%)','initial moisture content(%)', 'initial CN(%)', 'initial pH',\n",
    "\n",
    "for no, model in enumerate(loaded_models.values()):\n",
    "    # if no == 0:\n",
    "    #     continue\n",
    "    print(labels[no])\n",
    "    # 加载模型\n",
    "    loaded_model_1 = model\n",
    "    print(model)\n",
    "    print(loaded_models)\n",
    "    \n",
    "    # value_to_find = model # 替换为您要查找的值\n",
    "\n",
    "    # # 使用字典推导式查找对应的键\n",
    "    # keys = [key for key, value in loaded_models.items() if value == value_to_find]\n",
    "\n",
    "    # print(keys)\n",
    "\n",
    "\n",
    "    # 创建输入范围字典和输出范围字典\n",
    "    input_ranges_1 = {key: value for key, value in min_max_values.items() if key not in labels}\n",
    "    output_range_1 = {key: value for key, value in min_max_values.items() if key in labels}\n",
    "\n",
    "    print(\"Input Ranges:\")\n",
    "    print(input_ranges_1)\n",
    "    print(\"\\nOutput Ranges:\")\n",
    "    print(output_range_1)\n",
    "\n",
    "    # 创建属性名称到数值的映射\n",
    "    attribute_mapping = {key: idx for idx, key in enumerate(input_ranges_1.keys())}\n",
    "\n",
    "    # 最小化目标函数\n",
    "    creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "    toolbox = base.Toolbox()\n",
    "\n",
    "    # 定义生成整数属性的方法\n",
    "    toolbox.register(\"attr_int\", lambda minimum, maximum: random.randint(round(minimum), round(maximum)))\n",
    "\n",
    "    # 定义生成浮点数属性的方法\n",
    "    toolbox.register(\"attr_float\", lambda minimum, maximum: random.uniform(minimum, maximum))\n",
    "\n",
    "    # 创建个体时根据属性类型选择不同的生成方法\n",
    "    def create_individual():\n",
    "        individual = []\n",
    "        for attr_name, attr_info in input_ranges_1.items():\n",
    "            if attr_name in non_nums :\n",
    "                individual.append(toolbox.attr_int(attr_info['Minimum'], attr_info['Maximum']))\n",
    "            else:\n",
    "                individual.append(toolbox.attr_float(attr_info['Minimum'], attr_info['Maximum']))\n",
    "        return creator.Individual(individual)\n",
    "\n",
    "    # 注册个体生成方法\n",
    "    toolbox.register(\"individual\", create_individual)\n",
    "\n",
    "    # 初始化种群\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "    def evaluate(individual):\n",
    "        # 使用模型进行预测\n",
    "        individual_with_names = {attr_name: value for attr_name, value in zip(input_ranges_1.keys(), individual)}\n",
    "\n",
    "        # 检查每个特征值是否超出范围，如果超出范围则返回一个非常大的适应度值\n",
    "        for key, value in individual_with_names.items():\n",
    "            if key in non_nums:\n",
    "                if not isinstance(value, int) or value < input_ranges_1[key]['Minimum'] or value > input_ranges_1[key]['Maximum']:\n",
    "                    return (1e6,)  # 返回一个非常大的适应度值，表示不合法的个体\n",
    "            else:\n",
    "                if value < input_ranges_1[key]['Minimum'] or value > input_ranges_1[key]['Maximum']:\n",
    "                    return (1e6,)  # 返回一个非常大的适应度值，表示不合法的个体\n",
    "\n",
    "        # 使用模型进行预测\n",
    "        prediction = loaded_model_1.predict([individual])\n",
    "\n",
    "        # 计算模型输出并返回其负值作为适应度（因为我们是最小化问题）\n",
    "        fitness = -prediction\n",
    "        return fitness,\n",
    "\n",
    "    toolbox.register(\"evaluate\", evaluate)\n",
    "    toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)  # 交叉方式使用 Blend 交叉\n",
    "    # toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "\n",
    "    toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)  # 变异方式使用高斯变异\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "    # 进化算法\n",
    "    def main(population_size=100, n_generations=50, cxpb=0.5, mutpb=0.2):\n",
    "        pop = toolbox.population(n=population_size)\n",
    "        hof = tools.HallOfFame(1)\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"min\", min)\n",
    "\n",
    "        algorithms.eaSimple(pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=n_generations, stats=stats, halloffame=hof, verbose=False)\n",
    "\n",
    "        return pop, stats, hof\n",
    "\n",
    "    # 调整参数\n",
    "    population_size = 200\n",
    "    n_generations = 100\n",
    "    cxpb = 0.5  # 交叉概率\n",
    "    mutpb = 0.01  # 变异概率\n",
    "\n",
    "    num_runs = 100  # 运行次数\n",
    "    best_individuals = []\n",
    "\n",
    "    for i in range(num_runs):\n",
    "        pop, stats, hof = main(population_size, n_generations, cxpb, mutpb)\n",
    "        best_individual = tools.selBest(pop, 1)[0]\n",
    "        print(f\"Run {i+1}: Best individual with fitness {best_individual.fitness.values[0]}\")\n",
    "        best_individual_with_names = {list(input_ranges_1.keys())[idx]: value for idx, value in enumerate(best_individual)}\n",
    "        best_individuals.append((best_individual, best_individual_with_names))\n",
    "\n",
    "    # 保存最优个体到CSV文件\n",
    "    file_name = f\"best_individuals_{labels[no]}.csv\"\n",
    "    data = []\n",
    "    columns = list(input_ranges_1.keys()) + [labels[no]]\n",
    "    for best_individual, best_individual_with_names in best_individuals:\n",
    "        row = [best_individual_with_names[attr] for attr in input_ranges_1.keys()]\n",
    "        row.append(-best_individual.fitness.values[0])  # 添加适应度值\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "    print(f\"Best individuals saved to_{labels[no]}:\", file_name)\n",
    "\n",
    "\n",
    "    # 从保存的文件中找到标题为\"N2O-N loss (%)\"的最小值对应的组合\n",
    "    min_loss_combinations = []\n",
    "    min_loss = min(df[labels[no]])\n",
    "    for index, row in df.iterrows():\n",
    "        if row[labels[no]] == min_loss:\n",
    "            min_loss_combinations.append(row)\n",
    "\n",
    "    print(f\"\\nCombinations with minimum {labels[no]} ({min_loss}%):\")\n",
    "    for combination in min_loss_combinations:\n",
    "        print(combination)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从最优个体找出最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combinations with minimum N2O-N loss ([40.17083787]%):\n",
      "material_0                                       3\n",
      "initial TN(%)                            -0.355847\n",
      "initial TC(%)                            26.637128\n",
      "initial moisture content(%)              11.644503\n",
      "initial CN(%)                            19.048028\n",
      "initial pH                               10.547442\n",
      "material_1                                       3\n",
      "Excipients_1                             15.910138\n",
      "Additive Species                                 4\n",
      "N2O-N loss (%)                 [40.17083786807452]\n",
      "Name: 8, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 从保存的文件中找到标题为\"N2O-N loss (%)\"的最小值对应的组合\n",
    "min_N2O_loss_combinations = []\n",
    "min_N2O_loss = min(df['N2O-N loss (%)'])\n",
    "for index, row in df.iterrows():\n",
    "    if row['N2O-N loss (%)'] == min_N2O_loss:\n",
    "        min_N2O_loss_combinations.append(row)\n",
    "\n",
    "print(f\"\\nCombinations with minimum N2O-N loss ({min_N2O_loss}%):\")\n",
    "for combination in min_N2O_loss_combinations:\n",
    "    print(combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_NH3-N loss (%)_ctb_model优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ranges:\n",
      "{'material_0': {'Minimum': 0, 'Maximum': 6}, 'initial CN(%)': {'Minimum': -1.0, 'Maximum': 53.73}, 'initial moisture content(%)': {'Minimum': -1.0, 'Maximum': 89.8}, 'initial pH': {'Minimum': -1.0, 'Maximum': 10.7}, 'material_1': {'Minimum': 0, 'Maximum': 5}, 'Excipients': {'Minimum': 0, 'Maximum': 78}, 'initial TN(%)': {'Minimum': -1.0, 'Maximum': 14.56}, 'initial TC(%)': {'Minimum': -1.0, 'Maximum': 197.0}, 'Additive Species': {'Minimum': 0, 'Maximum': 4}}\n",
      "\n",
      "Output Ranges:\n",
      "{'NH3-N loss (%)': {'Minimum': 0.0, 'Maximum': 84.51}, 'TN loss (%)': {'Minimum': 0.2, 'Maximum': 90.5}}\n",
      "Run 1: Best individual with fitness [-19.99296867]\n",
      "Run 2: Best individual with fitness [-21.26771572]\n",
      "Run 3: Best individual with fitness [-23.0845398]\n",
      "Run 4: Best individual with fitness [-19.5720922]\n",
      "Run 5: Best individual with fitness [-21.73998721]\n",
      "Run 6: Best individual with fitness [-21.4707412]\n",
      "Run 7: Best individual with fitness [-24.56127793]\n",
      "Run 8: Best individual with fitness [-20.63281386]\n",
      "Run 9: Best individual with fitness [-17.80217724]\n",
      "Run 10: Best individual with fitness [-16.18912344]\n",
      "Run 11: Best individual with fitness [-18.62101058]\n",
      "Run 12: Best individual with fitness [-17.6144551]\n",
      "Run 13: Best individual with fitness [-18.12984574]\n",
      "Run 14: Best individual with fitness [-22.56179024]\n",
      "Run 15: Best individual with fitness [-20.43645637]\n",
      "Run 16: Best individual with fitness [-19.08559611]\n",
      "Run 17: Best individual with fitness [-17.96508636]\n",
      "Run 18: Best individual with fitness [-22.09225313]\n",
      "Run 19: Best individual with fitness [-23.65488018]\n",
      "Run 20: Best individual with fitness [-17.25653472]\n",
      "Run 21: Best individual with fitness [-20.21103789]\n",
      "Run 22: Best individual with fitness [-25.50367467]\n",
      "Run 23: Best individual with fitness [-21.82503478]\n",
      "Run 24: Best individual with fitness [-20.10138647]\n",
      "Run 25: Best individual with fitness [-26.74951067]\n",
      "Run 26: Best individual with fitness [-21.29825634]\n",
      "Run 27: Best individual with fitness [-16.02930523]\n",
      "Run 28: Best individual with fitness [-23.91361281]\n",
      "Run 29: Best individual with fitness [-28.22833353]\n",
      "Run 30: Best individual with fitness [-18.98959564]\n",
      "Run 31: Best individual with fitness [-18.343236]\n",
      "Run 32: Best individual with fitness [-17.53780652]\n",
      "Run 33: Best individual with fitness [-27.34231591]\n",
      "Run 34: Best individual with fitness [-17.63248795]\n",
      "Run 35: Best individual with fitness [-18.91918783]\n",
      "Run 36: Best individual with fitness [-21.68464127]\n",
      "Run 37: Best individual with fitness [-19.99673476]\n",
      "Run 38: Best individual with fitness [-18.85971253]\n",
      "Run 39: Best individual with fitness [-18.6834462]\n",
      "Run 40: Best individual with fitness [-20.14077138]\n",
      "Run 41: Best individual with fitness [-18.00168971]\n",
      "Run 42: Best individual with fitness [-18.05148024]\n",
      "Run 43: Best individual with fitness [-17.67575892]\n",
      "Run 44: Best individual with fitness [-17.42049141]\n",
      "Run 45: Best individual with fitness [-21.362671]\n",
      "Run 46: Best individual with fitness [-19.3806214]\n",
      "Run 47: Best individual with fitness [-23.13817659]\n",
      "Run 48: Best individual with fitness [-18.7862856]\n",
      "Run 49: Best individual with fitness [-20.17205674]\n",
      "Run 50: Best individual with fitness [-21.03358645]\n",
      "Run 51: Best individual with fitness [-17.83167236]\n",
      "Run 52: Best individual with fitness [-24.51952968]\n",
      "Run 53: Best individual with fitness [-23.9726313]\n",
      "Run 54: Best individual with fitness [-20.24467673]\n",
      "Run 55: Best individual with fitness [-19.99331817]\n",
      "Run 56: Best individual with fitness [-15.77313954]\n",
      "Run 57: Best individual with fitness [-19.4029992]\n",
      "Run 58: Best individual with fitness [-20.18894951]\n",
      "Run 59: Best individual with fitness [-21.07694203]\n",
      "Run 60: Best individual with fitness [-17.86726501]\n",
      "Run 61: Best individual with fitness [-21.65907146]\n",
      "Run 62: Best individual with fitness [-18.56855109]\n",
      "Run 63: Best individual with fitness [-19.97115259]\n",
      "Run 64: Best individual with fitness [-27.97791894]\n",
      "Run 65: Best individual with fitness [-20.89040947]\n",
      "Run 66: Best individual with fitness [-25.14826027]\n",
      "Run 67: Best individual with fitness [-22.25624091]\n",
      "Run 68: Best individual with fitness [-16.2846316]\n",
      "Run 69: Best individual with fitness [-18.05565344]\n",
      "Run 70: Best individual with fitness [-18.18370711]\n",
      "Run 71: Best individual with fitness [-22.89178759]\n",
      "Run 72: Best individual with fitness [-17.87467332]\n",
      "Run 73: Best individual with fitness [-17.23865377]\n",
      "Run 74: Best individual with fitness [-20.65292839]\n",
      "Run 75: Best individual with fitness [-22.76819406]\n",
      "Run 76: Best individual with fitness [-24.99281948]\n",
      "Run 77: Best individual with fitness [-26.87473472]\n",
      "Run 78: Best individual with fitness [-24.93513818]\n",
      "Run 79: Best individual with fitness [-30.30128104]\n",
      "Run 80: Best individual with fitness [-22.97982551]\n",
      "Run 81: Best individual with fitness [-16.80902669]\n",
      "Run 82: Best individual with fitness [-24.19850297]\n",
      "Run 83: Best individual with fitness [-19.32972919]\n",
      "Run 84: Best individual with fitness [-20.53420825]\n",
      "Run 85: Best individual with fitness [-18.60986132]\n",
      "Run 86: Best individual with fitness [-33.97744797]\n",
      "Run 87: Best individual with fitness [-18.87823147]\n",
      "Run 88: Best individual with fitness [-19.58608894]\n",
      "Run 89: Best individual with fitness [-23.47466061]\n",
      "Run 90: Best individual with fitness [-20.56227395]\n",
      "Run 91: Best individual with fitness [-20.33927804]\n",
      "Run 92: Best individual with fitness [-20.71468467]\n",
      "Run 93: Best individual with fitness [-22.18078712]\n",
      "Run 94: Best individual with fitness [-15.85772296]\n",
      "Run 95: Best individual with fitness [-26.69144261]\n",
      "Run 96: Best individual with fitness [-20.09257482]\n",
      "Run 97: Best individual with fitness [-23.11407328]\n",
      "Run 98: Best individual with fitness [-19.61681172]\n",
      "Run 99: Best individual with fitness [-22.27512033]\n",
      "Run 100: Best individual with fitness [-19.73450953]\n",
      "Best individuals saved to_NH3-N loss (%): best_individuals_NH3-N loss (%).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# 加载模型\n",
    "model_name_1 = 'model_NH3-N loss (%)_ctb_model'\n",
    "# 假设loaded_models已经定义，用于存储加载的模型\n",
    "# loaded_models = load_models() # 这里是你加载模型的代码，假设已经定义了\n",
    "loaded_model_1 = loaded_models[model_name_1]\n",
    "\n",
    "# 创建输入范围字典和输出范围字典\n",
    "input_ranges_1 = {key: value for key, value in min_max_values.items() if key not in ['N2O-N loss (%)', 'NH3-N loss (%)', 'TN loss (%)']}\n",
    "output_range_1 = {key: value for key, value in min_max_values.items() if key in ['NH3-N loss (%)', 'TN loss (%)']}\n",
    "\n",
    "print(\"Input Ranges:\")\n",
    "print(input_ranges_1)\n",
    "print(\"\\nOutput Ranges:\")\n",
    "print(output_range_1)\n",
    "\n",
    "# 创建属性名称到数值的映射\n",
    "attribute_mapping = {key: idx for idx, key in enumerate(input_ranges_1.keys())}\n",
    "\n",
    "# 最小化目标函数\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# 定义生成整数属性的方法\n",
    "toolbox.register(\"attr_int\", lambda minimum, maximum: random.randint(round(minimum), round(maximum)))\n",
    "\n",
    "# 定义生成浮点数属性的方法\n",
    "toolbox.register(\"attr_float\", lambda minimum, maximum: random.uniform(minimum, maximum))\n",
    "\n",
    "# 创建个体时根据属性类型选择不同的生成方法\n",
    "def create_individual():\n",
    "    individual = []\n",
    "    for attr_name, attr_info in input_ranges_1.items():\n",
    "        if attr_name in ['material_0', 'material_1', 'Excipients', 'Additive Species']:\n",
    "            individual.append(toolbox.attr_int(attr_info['Minimum'], attr_info['Maximum']))\n",
    "        else:\n",
    "            individual.append(toolbox.attr_float(attr_info['Minimum'], attr_info['Maximum']))\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "# 注册个体生成方法\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "\n",
    "# 初始化种群\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def evaluate(individual):\n",
    "    # 使用模型进行预测\n",
    "    individual_with_names = {attr_name: value for attr_name, value in zip(input_ranges_1.keys(), individual)}\n",
    "\n",
    "    # 检查每个特征值是否超出范围，如果超出范围则返回一个非常大的适应度值\n",
    "    for key, value in individual_with_names.items():\n",
    "        if key in ['material_0', 'material_1', 'Excipients', 'Additive Species']:\n",
    "            if not isinstance(value, int) or value < input_ranges_1[key]['Minimum'] or value > input_ranges_1[key]['Maximum']:\n",
    "                return (1e6,)  # 返回一个非常大的适应度值，表示不合法的个体\n",
    "        else:\n",
    "            if value < input_ranges_1[key]['Minimum'] or value > input_ranges_1[key]['Maximum']:\n",
    "                return (1e6,)  # 返回一个非常大的适应度值，表示不合法的个体\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    prediction = loaded_model_1.predict([individual])\n",
    "\n",
    "    # 计算模型输出并返回其负值作为适应度（因为我们是最小化问题）\n",
    "    fitness = -prediction\n",
    "    return fitness,\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)  # 交叉方式使用 Blend 交叉\n",
    "# toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)  # 变异方式使用高斯变异\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# 进化算法\n",
    "def main(population_size=100, n_generations=50, cxpb=0.5, mutpb=0.2):\n",
    "    pop = toolbox.population(n=population_size)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", min)\n",
    "\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=n_generations, stats=stats, halloffame=hof, verbose=False)\n",
    "\n",
    "    return pop, stats, hof\n",
    "\n",
    "# 调整参数\n",
    "population_size = 60\n",
    "n_generations = 100\n",
    "cxpb = 0.5  # 交叉概率\n",
    "mutpb = 0.01  # 变异概率\n",
    "\n",
    "num_runs = 100  # 运行次数\n",
    "best_individuals = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    pop, stats, hof = main(population_size, n_generations, cxpb, mutpb)\n",
    "    best_individual = tools.selBest(pop, 1)[0]\n",
    "    print(f\"Run {i+1}: Best individual with fitness {best_individual.fitness.values[0]}\")\n",
    "    best_individual_with_names = {list(input_ranges_1.keys())[idx]: value for idx, value in enumerate(best_individual)}\n",
    "    best_individuals.append((best_individual, best_individual_with_names))\n",
    "\n",
    "# 保存最优个体到CSV文件\n",
    "file_name = \"best_individuals_NH3-N loss (%).csv\"\n",
    "data = []\n",
    "columns = list(input_ranges_1.keys()) + [\"NH3-N loss (%)\"]\n",
    "for best_individual, best_individual_with_names in best_individuals:\n",
    "    row = [best_individual_with_names[attr] for attr in input_ranges_1.keys()]\n",
    "    row.append(-best_individual.fitness.values[0])  # 添加适应度值\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv(file_name, index=False)\n",
    "\n",
    "print(\"Best individuals saved to_NH3-N loss (%):\", file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从最优个体找出最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combinations with minimum N2O-N loss ([15.77313954]%):\n",
      "material_0                                        2\n",
      "initial CN(%)                              13.96201\n",
      "initial moisture content(%)               72.026239\n",
      "initial pH                                  2.11136\n",
      "material_1                                        5\n",
      "Excipients                                       39\n",
      "initial TN(%)                              6.427479\n",
      "initial TC(%)                            131.827844\n",
      "Additive Species                                  1\n",
      "NH3-N loss (%)                 [15.773139544012277]\n",
      "Name: 55, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 从保存的文件中找到标题为\"NH3-N loss (%)\"的最小值对应的组合\n",
    "min_NH3_N_loss_combinations = []\n",
    "min_NH3_N_loss = min(df['NH3-N loss (%)'])\n",
    "for index, row in df.iterrows():\n",
    "    if row['NH3-N loss (%)'] == min_NH3_N_loss:\n",
    "        min_NH3_N_loss_combinations.append(row)\n",
    "\n",
    "print(f\"\\nCombinations with minimum N2O-N loss ({min_NH3_N_loss}%):\")\n",
    "for combination in min_NH3_N_loss_combinations:\n",
    "    print(combination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_TN loss (%)_ctb_model优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ranges:\n",
      "{'material_0': {'Minimum': 0, 'Maximum': 6}, 'initial CN(%)': {'Minimum': -1.0, 'Maximum': 53.73}, 'initial moisture content(%)': {'Minimum': -1.0, 'Maximum': 89.8}, 'initial pH': {'Minimum': -1.0, 'Maximum': 10.7}, 'material_1': {'Minimum': 0, 'Maximum': 5}, 'Excipients': {'Minimum': 0, 'Maximum': 78}, 'initial TN(%)': {'Minimum': -1.0, 'Maximum': 14.56}, 'initial TC(%)': {'Minimum': -1.0, 'Maximum': 197.0}, 'Additive Species': {'Minimum': 0, 'Maximum': 4}}\n",
      "\n",
      "Output Ranges:\n",
      "{'TN loss (%)': {'Minimum': 0.2, 'Maximum': 90.5}}\n",
      "Run 1: Best individual with fitness [-37.97885911]\n",
      "Run 2: Best individual with fitness [-45.91499866]\n",
      "Run 3: Best individual with fitness [-40.86607202]\n",
      "Run 4: Best individual with fitness [-35.54169814]\n",
      "Run 5: Best individual with fitness [-46.21301536]\n",
      "Run 6: Best individual with fitness [-39.5328515]\n",
      "Run 7: Best individual with fitness [-39.05719107]\n",
      "Run 8: Best individual with fitness [-36.84531856]\n",
      "Run 9: Best individual with fitness [-43.6608655]\n",
      "Run 10: Best individual with fitness [-40.37948621]\n",
      "Run 11: Best individual with fitness [-34.56761079]\n",
      "Run 12: Best individual with fitness [-36.9280195]\n",
      "Run 13: Best individual with fitness [-44.42484145]\n",
      "Run 14: Best individual with fitness [-44.31846606]\n",
      "Run 15: Best individual with fitness [-37.9904211]\n",
      "Run 16: Best individual with fitness [-37.58079414]\n",
      "Run 17: Best individual with fitness [-43.71602918]\n",
      "Run 18: Best individual with fitness [-39.07582521]\n",
      "Run 19: Best individual with fitness [-46.4733819]\n",
      "Run 20: Best individual with fitness [-48.31520712]\n",
      "Run 21: Best individual with fitness [-43.56277411]\n",
      "Run 22: Best individual with fitness [-37.86737358]\n",
      "Run 23: Best individual with fitness [-38.04957204]\n",
      "Run 24: Best individual with fitness [-41.09243249]\n",
      "Run 25: Best individual with fitness [-43.73457695]\n",
      "Run 26: Best individual with fitness [-43.57313606]\n",
      "Run 27: Best individual with fitness [-42.67619672]\n",
      "Run 28: Best individual with fitness [-35.21131503]\n",
      "Run 29: Best individual with fitness [-37.16736083]\n",
      "Run 30: Best individual with fitness [-45.36319806]\n",
      "Run 31: Best individual with fitness [-41.50613142]\n",
      "Run 32: Best individual with fitness [-39.73491948]\n",
      "Run 33: Best individual with fitness [-33.15655421]\n",
      "Run 34: Best individual with fitness [-40.14427157]\n",
      "Run 35: Best individual with fitness [-35.99616884]\n",
      "Run 36: Best individual with fitness [-69.39415503]\n",
      "Run 37: Best individual with fitness [-40.05626319]\n",
      "Run 38: Best individual with fitness [-38.0997779]\n",
      "Run 39: Best individual with fitness [-43.59598727]\n",
      "Run 40: Best individual with fitness [-40.53544699]\n",
      "Run 41: Best individual with fitness [-34.73632286]\n",
      "Run 42: Best individual with fitness [-36.93915873]\n",
      "Run 43: Best individual with fitness [-37.4903396]\n",
      "Run 44: Best individual with fitness [-37.55784044]\n",
      "Run 45: Best individual with fitness [-47.35969172]\n",
      "Run 46: Best individual with fitness [-44.8875008]\n",
      "Run 47: Best individual with fitness [-33.46956511]\n",
      "Run 48: Best individual with fitness [-37.25852496]\n",
      "Run 49: Best individual with fitness [-36.46001943]\n",
      "Run 50: Best individual with fitness [-44.10426564]\n",
      "Run 51: Best individual with fitness [-40.9850401]\n",
      "Run 52: Best individual with fitness [-34.92925292]\n",
      "Run 53: Best individual with fitness [-43.21674511]\n",
      "Run 54: Best individual with fitness [-34.85670212]\n",
      "Run 55: Best individual with fitness [-42.58130828]\n",
      "Run 56: Best individual with fitness [-39.96067747]\n",
      "Run 57: Best individual with fitness [-37.47475588]\n",
      "Run 58: Best individual with fitness [-39.29148276]\n",
      "Run 59: Best individual with fitness [-32.24972531]\n",
      "Run 60: Best individual with fitness [-38.653587]\n",
      "Run 61: Best individual with fitness [-45.61310965]\n",
      "Run 62: Best individual with fitness [-37.41102902]\n",
      "Run 63: Best individual with fitness [-47.21170068]\n",
      "Run 64: Best individual with fitness [-39.10594968]\n",
      "Run 65: Best individual with fitness [-44.41339096]\n",
      "Run 66: Best individual with fitness [-39.86222107]\n",
      "Run 67: Best individual with fitness [-44.66560822]\n",
      "Run 68: Best individual with fitness [-38.95506069]\n",
      "Run 69: Best individual with fitness [-44.93043292]\n",
      "Run 70: Best individual with fitness [-37.76328173]\n",
      "Run 71: Best individual with fitness [-44.02664137]\n",
      "Run 72: Best individual with fitness [-40.28533271]\n",
      "Run 73: Best individual with fitness [-36.99754629]\n",
      "Run 74: Best individual with fitness [-46.09379453]\n",
      "Run 75: Best individual with fitness [-35.5279379]\n",
      "Run 76: Best individual with fitness [-42.73640407]\n",
      "Run 77: Best individual with fitness [-39.08223714]\n",
      "Run 78: Best individual with fitness [-47.34326348]\n",
      "Run 79: Best individual with fitness [-37.7923774]\n",
      "Run 80: Best individual with fitness [-35.76059991]\n",
      "Run 81: Best individual with fitness [-37.34644579]\n",
      "Run 82: Best individual with fitness [-38.84814925]\n",
      "Run 83: Best individual with fitness [-44.17394972]\n",
      "Run 84: Best individual with fitness [-39.56417195]\n",
      "Run 85: Best individual with fitness [-41.18881527]\n",
      "Run 86: Best individual with fitness [-36.03880243]\n",
      "Run 87: Best individual with fitness [-52.44014715]\n",
      "Run 88: Best individual with fitness [-37.12629645]\n",
      "Run 89: Best individual with fitness [-43.53570159]\n",
      "Run 90: Best individual with fitness [-40.56449443]\n",
      "Run 91: Best individual with fitness [-44.89002113]\n",
      "Run 92: Best individual with fitness [-53.15493917]\n",
      "Run 93: Best individual with fitness [-47.0542908]\n",
      "Run 94: Best individual with fitness [-40.03500336]\n",
      "Run 95: Best individual with fitness [-44.13044298]\n",
      "Run 96: Best individual with fitness [-36.67233958]\n",
      "Run 97: Best individual with fitness [-46.78937701]\n",
      "Run 98: Best individual with fitness [-38.31113644]\n",
      "Run 99: Best individual with fitness [-60.69309051]\n",
      "Run 100: Best individual with fitness [-48.36700983]\n",
      "Best individuals saved to_TN loss (%): best_individuals_TN loss (%).csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "\n",
    "# 加载模型\n",
    "model_name_1 = 'model_TN loss (%)_ctb_model'\n",
    "# 假设loaded_models已经定义，用于存储加载的模型\n",
    "# loaded_models = load_models() # 这里是你加载模型的代码，假设已经定义了\n",
    "loaded_model_1 = loaded_models[model_name_1]\n",
    "\n",
    "# 创建输入范围字典和输出范围字典\n",
    "input_ranges_1 = {key: value for key, value in min_max_values.items() if key not in ['N2O-N loss (%)', 'NH3-N loss (%)', 'TN loss (%)']}\n",
    "output_range_1 = {key: value for key, value in min_max_values.items() if key in ['TN loss (%)']}\n",
    "\n",
    "print(\"Input Ranges:\")\n",
    "print(input_ranges_1)\n",
    "print(\"\\nOutput Ranges:\")\n",
    "print(output_range_1)\n",
    "\n",
    "# 创建属性名称到数值的映射\n",
    "attribute_mapping = {key: idx for idx, key in enumerate(input_ranges_1.keys())}\n",
    "\n",
    "# 最小化目标函数\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# 定义生成整数属性的方法\n",
    "toolbox.register(\"attr_int\", lambda minimum, maximum: random.randint(round(minimum), round(maximum)))\n",
    "\n",
    "# 定义生成浮点数属性的方法\n",
    "toolbox.register(\"attr_float\", lambda minimum, maximum: random.uniform(minimum, maximum))\n",
    "\n",
    "# 创建个体时根据属性类型选择不同的生成方法\n",
    "def create_individual():\n",
    "    individual = []\n",
    "    for attr_name, attr_info in input_ranges_1.items():\n",
    "        if attr_name in ['material_0', 'material_1', 'Excipients', 'Additive Species']:\n",
    "            individual.append(toolbox.attr_int(attr_info['Minimum'], attr_info['Maximum']))\n",
    "        else:\n",
    "            individual.append(toolbox.attr_float(attr_info['Minimum'], attr_info['Maximum']))\n",
    "    return creator.Individual(individual)\n",
    "\n",
    "# 注册个体生成方法\n",
    "toolbox.register(\"individual\", create_individual)\n",
    "\n",
    "# 初始化种群\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def evaluate(individual):\n",
    "    # 使用模型进行预测\n",
    "    individual_with_names = {attr_name: value for attr_name, value in zip(input_ranges_1.keys(), individual)}\n",
    "\n",
    "    # 检查每个特征值是否超出范围，如果超出范围则返回一个非常大的适应度值\n",
    "    for key, value in individual_with_names.items():\n",
    "        if key in ['material_0', 'material_1', 'Excipients', 'Additive Species']:\n",
    "            if not isinstance(value, int) or value < input_ranges_1[key]['Minimum'] or value > input_ranges_1[key]['Maximum']:\n",
    "                return (1e6,)  # 返回一个非常大的适应度值，表示不合法的个体\n",
    "        else:\n",
    "            if value < input_ranges_1[key]['Minimum'] or value > input_ranges_1[key]['Maximum']:\n",
    "                return (1e6,)  # 返回一个非常大的适应度值，表示不合法的个体\n",
    "\n",
    "    # 使用模型进行预测\n",
    "    prediction = loaded_model_1.predict([individual])\n",
    "\n",
    "    # 计算模型输出并返回其负值作为适应度（因为我们是最小化问题）\n",
    "    fitness = -prediction\n",
    "    return fitness,\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxBlend, alpha=0.5)  # 交叉方式使用 Blend 交叉\n",
    "# toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=1, indpb=0.1)  # 变异方式使用高斯变异\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# 进化算法\n",
    "def main(population_size=100, n_generations=50, cxpb=0.5, mutpb=0.2):\n",
    "    pop = toolbox.population(n=population_size)\n",
    "    hof = tools.HallOfFame(1)\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"min\", min)\n",
    "\n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=cxpb, mutpb=mutpb, ngen=n_generations, stats=stats, halloffame=hof, verbose=False)\n",
    "\n",
    "    return pop, stats, hof\n",
    "\n",
    "# 调整参数\n",
    "population_size = 100\n",
    "n_generations = 50\n",
    "cxpb = 0.3  # 交叉概率\n",
    "mutpb = 0.02  # 变异概率\n",
    "\n",
    "num_runs = 100  # 运行次数\n",
    "best_individuals = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    pop, stats, hof = main(population_size, n_generations, cxpb, mutpb)\n",
    "    best_individual = tools.selBest(pop, 1)[0]\n",
    "    print(f\"Run {i+1}: Best individual with fitness {best_individual.fitness.values[0]}\")\n",
    "    best_individual_with_names = {list(input_ranges_1.keys())[idx]: value for idx, value in enumerate(best_individual)}\n",
    "    best_individuals.append((best_individual, best_individual_with_names))\n",
    "\n",
    "# 保存最优个体到CSV文件\n",
    "file_name = \"best_individuals_TN loss (%).csv\"\n",
    "data = []\n",
    "columns = list(input_ranges_1.keys()) + [\"TN loss (%)\"]\n",
    "for best_individual, best_individual_with_names in best_individuals:\n",
    "    row = [best_individual_with_names[attr] for attr in input_ranges_1.keys()]\n",
    "    row.append(-best_individual.fitness.values[0])  # 添加适应度值\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "df.to_csv(file_name, index=False)\n",
    "\n",
    "print(\"Best individuals saved to_TN loss (%):\", file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从最优个体找出最小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combinations with minimum N2O-N loss ([32.24972531]%):\n",
      "material_0                                       1\n",
      "initial CN(%)                              4.42864\n",
      "initial moisture content(%)               33.91675\n",
      "initial pH                                9.506107\n",
      "material_1                                       3\n",
      "Excipients                                      52\n",
      "initial TN(%)                            11.699848\n",
      "initial TC(%)                            79.116854\n",
      "Additive Species                                 4\n",
      "TN loss (%)                    [32.24972530675092]\n",
      "Name: 58, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 从保存的文件中找到标题为\"TN loss (%)\"的最小值对应的组合\n",
    "min_TN_loss_combinations = []\n",
    "min_TN_loss = min(df['TN loss (%)'])\n",
    "for index, row in df.iterrows():\n",
    "    if row['TN loss (%)'] == min_TN_loss:\n",
    "        min_TN_loss_combinations.append(row)\n",
    "\n",
    "print(f\"\\nCombinations with minimum N2O-N loss ({min_TN_loss}%):\")\n",
    "for combination in min_TN_loss_combinations:\n",
    "    print(combination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
